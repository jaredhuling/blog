<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.30.2" />


<title>The oem package for penalized regression is on CRAN - Jared Huling</title>
<meta property="og:title" content="The oem package for penalized regression is on CRAN - Jared Huling">



  








<link href='//cdn.bootcss.com/highlight.js/9.11.0/styles/arta.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="../../../../css/fonts.css" media="all">
<link rel="stylesheet" href="../../../../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../../../../" class="nav-logo">
    <img src="../../../../images/%3cnil%3e"
         width=""
         height=""
         alt="">
  </a>

  <ul class="nav-links">
    
    <li><a href="../../../../about/">About</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">11 min read</span>
    

    <h1 class="article-title">The oem package for penalized regression is on CRAN</h1>

    
    <span class="article-date">2017/07/19</span>
    

    <div class="article-content">
      <p>The <a href="https://cran.r-project.org/package=oem">oem</a> package has been on CRAN for some time now, but with the latest update I expect few structural changes to the user interface. <code>oem</code> is a package for the estimation of various penalized regression models using the oem algorithm of <a href="http://amstat.tandfonline.com/doi/abs/10.1080/00401706.2015.1054436">Xiong et al. (2016)</a>. The focus of <code>oem</code> is to provide high performance computation for big <strong>tall</strong> data. Many applications not only have a large number of variables, but a vast number of observations; <code>oem</code> is designed to perform well in these settings.</p>
<ul>
<li>Fast computation for big <strong>tall</strong> data</li>
<li>Efficient computation for computation for multiple penalties simultaneously</li>
<li>Efficient cross-validation</li>
</ul>
<p>In this post I’ll give a brief overview of what is in the <code>oem</code> package and how to use it.</p>
<div id="installation" class="section level1">
<h1>Installation</h1>
<p>The simplest way to install <code>oem</code> is via the CRAN repositories as the following:</p>
<pre class="r"><code>install.packages(&quot;oem&quot;)</code></pre>
<p>To install the development version, first install the <code>devtools</code> package</p>
<pre class="r"><code>install.packages(&quot;devtools&quot;)</code></pre>
<p>and then install <code>oem</code> via the <code>install_github</code> function</p>
<pre class="r"><code>devtools::install_github(&quot;jaredhuling/oem&quot;)</code></pre>
</div>
<div id="quick-start" class="section level1">
<h1>Quick Start</h1>
<p>First load <code>oem</code></p>
<pre class="r"><code>library(oem)</code></pre>
<p>Simulate data</p>
<pre class="r"><code>nobs  &lt;- 1e4
nvars &lt;- 100
x &lt;- matrix(rnorm(nobs * nvars), ncol = nvars)
y &lt;- drop(x %*% c(0.5, 0.5, -0.5, -0.5, 1, rep(0, nvars - 5))) + rnorm(nobs, sd = 4)</code></pre>
<p>Fit a penalized regression model using the <code>oem</code> function</p>
<pre class="r"><code>fit1 &lt;- oem(x = x, y = y, penalty = &quot;lasso&quot;)</code></pre>
<p>Plot the solution path</p>
<pre class="r"><code>plot(fit1)</code></pre>
<p><img src="../../../../post/2017-07-19-oem_cran_files/figure-html/unnamed-chunk-7-1.png" width="686.4" /></p>
</div>
<div id="key-features" class="section level1">
<h1>Key Features</h1>
<div id="available-functions" class="section level2">
<h2>Available functions</h2>
<table>
<thead>
<tr class="header">
<th align="left">Function Name</th>
<th align="left">Functionality</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>oem()</code></td>
<td align="left">Main fitting function</td>
</tr>
<tr class="even">
<td align="left"><code>predict.oemfit()</code></td>
<td align="left">Prediction for oem objects</td>
</tr>
<tr class="odd">
<td align="left"><code>plot.oemfit()</code></td>
<td align="left">Plotting for oem objects</td>
</tr>
<tr class="even">
<td align="left"><code>logLik.oemfit()</code></td>
<td align="left">log Likelihood for oem objects</td>
</tr>
<tr class="odd">
<td align="left"><code>cv.oem()</code></td>
<td align="left">Cross-validation function</td>
</tr>
<tr class="even">
<td align="left"><code>xval.oem()</code></td>
<td align="left">Accelerated cross-validation for linear models</td>
</tr>
<tr class="odd">
<td align="left"><code>predict.cv.oem()</code></td>
<td align="left">Prediction for cv.oem objects</td>
</tr>
<tr class="even">
<td align="left"><code>plot.cv.oem()</code></td>
<td align="left">Plotting for cv.oem objects</td>
</tr>
<tr class="odd">
<td align="left"><code>logLik.cv.oem()</code></td>
<td align="left">log Likelihood for cv.oem objects</td>
</tr>
</tbody>
</table>
</div>
<div id="available-penalties" class="section level2">
<h2>Available Penalties</h2>
<table style="width:96%;">
<colgroup>
<col width="29%" />
<col width="29%" />
<col width="37%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Penalty</th>
<th align="left">Option Name</th>
<th align="left">Penalty Form</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Lasso</td>
<td align="left"><code>lasso</code></td>
<td align="left"><span class="math inline">\(\lambda \sum_{j = 1}^pd_j\vert\beta_j\vert\)</span></td>
</tr>
<tr class="even">
<td align="left">Elastic Net</td>
<td align="left"><code>elastic.net</code></td>
<td align="left"><span class="math inline">\(\lambda \sum_{j = 1}^p\alpha d_j\vert\beta_j\vert + (1 - \alpha)\lambda \sum_{j = 1}^pd_j\beta_j^2\)</span></td>
</tr>
<tr class="odd">
<td align="left">MCP</td>
<td align="left"><code>mcp</code></td>
<td align="left"><span class="math inline">\(\lambda \sum_{j = 1}^pd_j \int_0^{\beta_j}(1 - x/(\gamma\lambda d_j))_+\mathrm{d}x\)</span></td>
</tr>
<tr class="even">
<td align="left">SCAD</td>
<td align="left"><code>scad</code></td>
<td align="left"><span class="math inline">\(\sum_{j = 1}^p p^{SCAD}_{\lambda d_j,\gamma}(\beta_j)\)</span></td>
</tr>
<tr class="odd">
<td align="left">Group Lasso</td>
<td align="left"><code>grp.lasso</code></td>
<td align="left"><span class="math inline">\(\lambda \sum_{k = 1}^Gd_k\sqrt{\sum_{j \in g_k}\beta_j^2}\)</span></td>
</tr>
<tr class="even">
<td align="left">Group MCP</td>
<td align="left"><code>grp.mcp</code></td>
<td align="left"><span class="math inline">\(\sum_{k = 1}^G p_{\lambda d_k,\gamma}^{MCP}\left(\vert \vert \boldsymbol\beta_{g_k}\vert \vert_2\right)\)</span></td>
</tr>
<tr class="odd">
<td align="left">Group SCAD</td>
<td align="left"><code>grp.scad</code></td>
<td align="left"><span class="math inline">\(\sum_{k = 1}^G p_{\lambda d_k,\gamma}^{SCAD}\left(\vert \vert \boldsymbol\beta_{g_k}\vert \vert_2\right)\)</span></td>
</tr>
<tr class="even">
<td align="left">Sparse Group Lasso</td>
<td align="left"><code>sparse.grp.lasso</code></td>
<td align="left"><span class="math inline">\(\lambda \alpha\sum_{j = 1}^pd_j\vert\beta_j\vert + \lambda (1-\alpha)\sum_{k = 1}^Gd_k\sqrt{\sum_{j \in g_k}\beta_j^2}\)</span></td>
</tr>
</tbody>
</table>
<p>where <span class="math inline">\(\vert\vert\boldsymbol\beta_{g_k}\vert\vert_2 = \sqrt{\sum_{j \in g_k}\beta_j^2}\)</span>.</p>
<p>Any penalty with <code>.net</code> at the end of its name has a ridge term of <span class="math inline">\((1 - \alpha)\lambda \sum_{j = 1}^pd_j\beta_j^2\)</span> added to it and the original penalty multiplied by <span class="math inline">\(\alpha\)</span>. For example, <code>grp.mcp.net</code> is the penalty</p>
<p><span class="math display">\[\lambda \sum_{k = 1}^G\alpha p^{MCP}_{\lambda d_k,\gamma}(\vert\vert\boldsymbol\beta_{g_k}\vert\vert_2) + (1 - \alpha)\lambda \sum_{j = 1}^pd_j\beta_j^2.
\]</span></p>
</div>
<div id="available-model-families" class="section level2">
<h2>Available Model Families</h2>
<p>The following models are available currently.</p>
<table style="width:97%;">
<colgroup>
<col width="30%" />
<col width="29%" />
<col width="37%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Model</th>
<th align="left">Option Name</th>
<th align="left">Loss Form</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Linear Regression</td>
<td align="left"><code>gaussian</code></td>
<td align="left"><span class="math inline">\(\frac{1}{2n}\sum_{i=1}^n(y_i - x_i^T\beta) ^ 2\)</span></td>
</tr>
<tr class="even">
<td align="left">Logistic Regression</td>
<td align="left"><code>binomial</code></td>
<td align="left"><span class="math inline">\(-\frac{1}{n}\sum_{i=1}^n\left[y_i x_i^T\beta - \log (1 + \exp\{ x_i^T\beta \} ) \right]\)</span></td>
</tr>
</tbody>
</table>
<p>There are plans to include support for multiple responses, binomial models (not just logistic regression), Cox’s proportional hazards model, and more if requested.</p>
</div>
</div>
<div id="fitting-multiple-penalties-at-once" class="section level1">
<h1>Fitting multiple penalties at once</h1>
<p>The oem algorithm is well-suited to quickly estimate a solution path for multiple penalties simultaneously if the number of variables is not too large. The oem algorithm is only efficient for multiple penalties for linear models.</p>
<p>For the group lasso penalty, the <code>groups</code> argument must be used. <code>groups</code> should be a vector which indicates the group number for each variable.</p>
<pre class="r"><code>fit2 &lt;- oem(x = x, y = y, penalty = c(&quot;lasso&quot;, &quot;mcp&quot;, &quot;grp.lasso&quot;, &quot;grp.mcp&quot;),
            groups = rep(1:20, each = 5))</code></pre>
<p>Plot the solution paths for all models</p>
<pre class="r"><code>layout(matrix(1:4, ncol = 2))
plot(fit2, which.model = 1, xvar = &quot;lambda&quot;)
plot(fit2, which.model = 2, xvar = &quot;lambda&quot;)
plot(fit2, which.model = 3, xvar = &quot;lambda&quot;)
plot(fit2, which.model = &quot;grp.mcp&quot;, xvar = &quot;lambda&quot;)</code></pre>
<p><img src="../../../../post/2017-07-19-oem_cran_files/figure-html/unnamed-chunk-9-1.png" width="686.4" /></p>
<div id="timing-comparison" class="section level2">
<h2>Timing Comparison</h2>
<p>The following is a demonstration of oem’s efficiency for computing solutions for tuning parameter paths for multiple penalties at once.</p>
<div id="linear-regression" class="section level3">
<h3>Linear Regression</h3>
<p>The efficiency oem for fitting multiple penalties at once only applies to linear models. However, for linear models it is quite efficient, even for a high number of tuning parameters for many different penalties.</p>
<pre class="r"><code>nobs  &lt;- 1e5
nvars &lt;- 100
x2 &lt;- matrix(rnorm(nobs * nvars), ncol = nvars)
y2 &lt;- drop(x2 %*% c(0.5, 0.5, -0.5, -0.5, 1, rep(0, nvars - 5))) + rnorm(nobs, sd = 4)

system.time(fit2a &lt;- oem(x = x2, y = y2, penalty = c(&quot;grp.lasso&quot;),
                         groups = rep(1:20, each = 5), nlambda = 100L))</code></pre>
<pre><code>##    user  system elapsed 
##   0.139   0.016   0.156</code></pre>
<pre class="r"><code>system.time(fit2b &lt;- oem(x = x2, y = y2, 
                         penalty = c(&quot;grp.lasso&quot;, &quot;lasso&quot;, &quot;mcp&quot;, 
                                     &quot;scad&quot;, &quot;elastic.net&quot;, &quot;grp.mcp&quot;,
                                     &quot;grp.scad&quot;, &quot;sparse.grp.lasso&quot;),
                         groups = rep(1:20, each = 5), nlambda = 100L))</code></pre>
<pre><code>##    user  system elapsed 
##   0.159   0.016   0.176</code></pre>
<pre class="r"><code>system.time(fit2c &lt;- oem(x = x2, y = y2, 
                         penalty = c(&quot;grp.lasso&quot;, &quot;lasso&quot;, &quot;mcp&quot;, 
                                     &quot;scad&quot;, &quot;elastic.net&quot;, &quot;grp.mcp&quot;,
                                     &quot;grp.scad&quot;, &quot;sparse.grp.lasso&quot;),
                         groups = rep(1:20, each = 5), nlambda = 500L))</code></pre>
<pre><code>##    user  system elapsed 
##   0.228   0.023   0.250</code></pre>
</div>
<div id="logistic-regression" class="section level3">
<h3>Logistic Regression</h3>
<p>It is still more efficient to fit multiple penalties at once instead of individually for logistic regression, but the benefit is not as dramatic as for linear models.</p>
<pre class="r"><code>nobs  &lt;- 5e4
nvars &lt;- 100
x2 &lt;- matrix(rnorm(nobs * nvars), ncol = nvars)

y2 &lt;- rbinom(nobs, 1, prob = 1 / (1 + exp(-drop(x2 %*% c(0.15, 0.15, -0.15, -0.15, 0.25, rep(0, nvars - 5))))))


system.time(fit2a &lt;- oem(x = x2, y = y2, penalty = c(&quot;grp.lasso&quot;),
                         family = &quot;binomial&quot;,
                         groups = rep(1:20, each = 5), nlambda = 100L))</code></pre>
<pre><code>##    user  system elapsed 
##   0.958   0.035   1.000</code></pre>
<pre class="r"><code>system.time(fit2b &lt;- oem(x = x2, y = y2, penalty = c(&quot;grp.lasso&quot;, &quot;lasso&quot;, &quot;mcp&quot;, &quot;scad&quot;, &quot;elastic.net&quot;),
                         family = &quot;binomial&quot;,
                         groups = rep(1:20, each = 5), nlambda = 100L))</code></pre>
<pre><code>##    user  system elapsed 
##   4.997   0.072   5.093</code></pre>
</div>
</div>
</div>
<div id="cross-validation" class="section level1">
<h1>Cross Validation</h1>
<p>Here we use the <code>nfolds</code> argument to specify the number of folds for <span class="math inline">\(k\)</span>-fold cross validation</p>
<pre class="r"><code>system.time(cvfit1 &lt;- cv.oem(x = x, y = y, 
                             penalty = c(&quot;lasso&quot;, &quot;mcp&quot;, 
                                         &quot;grp.lasso&quot;, &quot;grp.mcp&quot;), 
                             gamma = 2,
                             groups = rep(1:20, each = 5), 
                             nfolds = 10))</code></pre>
<pre><code>##    user  system elapsed 
##   1.171   0.225   1.400</code></pre>
<p>Plot the cross validation mean squared error results for each model</p>
<pre class="r"><code>layout(matrix(1:4, ncol = 2))
plot(cvfit1, which.model = 1)
plot(cvfit1, which.model = 2)
plot(cvfit1, which.model = 3)
plot(cvfit1, which.model = 4)</code></pre>
<p><img src="../../../../post/2017-07-19-oem_cran_files/figure-html/unnamed-chunk-13-1.png" width="686.4" /></p>
<div id="extremely-fast-cross-validation-for-linear-models" class="section level2">
<h2>Extremely Fast Cross Validation for Linear Models</h2>
<p>The function <code>xval.oem</code> offers accelerated cross validation for penalized linear models. In many cases is is orders of magnitude faster than cv.oem. It is only recommended for scenarios where the number of observations is larger than the number of variables. In addition to the computational gains in single-core usage, it also benefits from parallelizaton using OpenMP (instead of using foreach, as used by cv.oem). For large enough problems, it has on a similar order of computation time as just fitting one OEM model.</p>
<pre class="r"><code>nobsc  &lt;- 1e5
nvarsc &lt;- 100
xc &lt;- matrix(rnorm(nobsc * nvarsc), ncol = nvarsc)
yc &lt;- drop(xc %*% c(0.5, 0.5, -0.5, -0.5, 1, rep(0, nvarsc - 5))) + rnorm(nobsc, sd = 4)

system.time(cvalfit1 &lt;- cv.oem(x = xc, y = yc, penalty = &quot;lasso&quot;, 
                               groups = rep(1:20, each = 5), 
                               nfolds = 10))</code></pre>
<pre><code>##    user  system elapsed 
##   4.392   0.990   5.400</code></pre>
<pre class="r"><code>system.time(xvalfit1 &lt;- xval.oem(x = xc, y = yc, penalty = &quot;lasso&quot;,
                                 groups = rep(1:20, each = 5), 
                                 nfolds = 10))</code></pre>
<pre><code>##    user  system elapsed 
##   0.538   0.021   0.562</code></pre>
<pre class="r"><code>system.time(xvalfit2 &lt;- xval.oem(x = xc, y = yc, penalty = &quot;lasso&quot;,
                                 groups = rep(1:20, each = 5), 
                                 nfolds = 10, ncores = 2))</code></pre>
<pre><code>##    user  system elapsed 
##   0.657   0.020   0.395</code></pre>
<pre class="r"><code>system.time(ofit1 &lt;- oem(x = xc, y = yc, penalty = &quot;lasso&quot;,
                         groups = rep(1:20, each = 5)))</code></pre>
<pre><code>##    user  system elapsed 
##   0.229   0.015   0.143</code></pre>
</div>
<div id="evaluation-metrics" class="section level2">
<h2>Evaluation Metrics</h2>
<p>A variety of evaluation metrics can be used for cross validation. The available metrics can be found in the table below</p>
<table>
<thead>
<tr class="header">
<th align="left">Model</th>
<th align="left">Metric</th>
<th align="left"><code>type.measure=</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>Linear Regression</strong></td>
<td align="left">Mean squared error</td>
<td align="left"><code>mse</code> or <code>deviance</code></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left">Mean absolute error</td>
<td align="left"><code>mae</code></td>
</tr>
<tr class="odd">
<td align="left"><strong>Logistic Regression</strong></td>
<td align="left">Deviance</td>
<td align="left"><code>deviance</code></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left">Area under the ROC curve</td>
<td align="left"><code>auc</code></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left">Misclassification Rate</td>
<td align="left"><code>class</code></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left">Mean squared error of fitted mean</td>
<td align="left"><code>mse</code></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left">Mean absolute error of fitted mean</td>
<td align="left"><code>mae</code></td>
</tr>
</tbody>
</table>
<p>Consider a binary outcome setting with logistic regression.</p>
<pre class="r"><code>nobs  &lt;- 2e3
nvars &lt;- 20
x &lt;- matrix(runif(nobs * nvars, max = 2), ncol = nvars)

y &lt;- rbinom(nobs, 1, prob = 1 / (1 + exp(-drop(x %*% c(0.25, -1, -1, -0.5, -0.5, -0.25, rep(0, nvars - 6))))))</code></pre>
<div id="misclassification-rate" class="section level3">
<h3>Misclassification Rate</h3>
<pre class="r"><code>cvfit2 &lt;- cv.oem(x = x, y = y, penalty = c(&quot;lasso&quot;, &quot;mcp&quot;, 
                                           &quot;grp.lasso&quot;, &quot;grp.mcp&quot;), 
                 family = &quot;binomial&quot;,
                 type.measure = &quot;class&quot;,
                 gamma = 2,
                 groups = rep(1:10, each = 2), 
                 nfolds = 10, standardize = FALSE)</code></pre>
<p><img src="../../../../post/2017-07-19-oem_cran_files/figure-html/unnamed-chunk-17-1.png" width="686.4" /></p>
<p>In this case, misclassification rate is not the best indicator of performance. The classes here are imbalanced:</p>
<pre class="r"><code>mean(y)</code></pre>
<pre><code>## [1] 0.0695</code></pre>
</div>
<div id="area-under-the-roc-curve" class="section level3">
<h3>Area Under the ROC Curve</h3>
<p>Area under the ROC curve is an alternative classification metric to misclassification rate. It is available by setting <code>type.measure = &quot;auc&quot;</code>.</p>
<pre class="r"><code>cvfit2 &lt;- cv.oem(x = x, y = y, penalty = c(&quot;lasso&quot;, &quot;mcp&quot;, 
                                           &quot;grp.lasso&quot;, &quot;grp.mcp&quot;), 
                 family = &quot;binomial&quot;,
                 type.measure = &quot;auc&quot;,
                 gamma = 2,
                 groups = rep(1:10, each = 2), 
                 nfolds = 10, standardize = FALSE)</code></pre>
<p><img src="../../../../post/2017-07-19-oem_cran_files/figure-html/unnamed-chunk-20-1.png" width="686.4" /></p>
</div>
</div>
</div>
<div id="methods-for-very-large-scale-problems" class="section level1">
<h1>Methods for Very Large Scale Problems</h1>
<div id="oem-with-precomputed-xtx-xty-for-linear-models" class="section level2">
<h2>OEM with Precomputed <span class="math inline">\(X^TX\)</span>, <span class="math inline">\(X^TY\)</span> for Linear Models</h2>
<p>With a very large dataset and computing cluster, the total size of a dataset may be very large, but if the number of variables is only moderately large (on the order of a few thousands) <span class="math inline">\(X^TX\)</span> and <span class="math inline">\(X^TY\)</span> may not be large and may already be available from other computations or can be computed trivially in parallel. The function <code>oem.xtx</code> computes penalized linear regression models using the OEM algorithm only using <span class="math inline">\(X^TX\)</span> and <span class="math inline">\(X^TY\)</span>. Standardization can be achieved by providing a vector of scaling factors (usually the standard deviations of the columns of x). The function is used like the following:</p>
<pre class="r"><code>xtx &lt;- crossprod(xc) / nrow(xc)
xty &lt;- crossprod(xc, yc) / nrow(xc)


system.time(fit &lt;- oem(x = xc, y = yc, 
                       penalty = c(&quot;lasso&quot;, &quot;grp.lasso&quot;), 
                       standardize = FALSE, intercept = FALSE,
                       groups = rep(1:20, each = 5)))</code></pre>
<pre><code>##    user  system elapsed 
##   0.131   0.033   0.164</code></pre>
<pre class="r"><code>system.time(fit.xtx &lt;- oem.xtx(xtx = xtx, xty = xty, 
                               penalty = c(&quot;lasso&quot;, &quot;grp.lasso&quot;), 
                               groups = rep(1:20, each = 5))  )  </code></pre>
<pre><code>##    user  system elapsed 
##   0.008   0.000   0.009</code></pre>
<pre class="r"><code>max(abs(fit$beta[[1]][-1,] - fit.xtx$beta[[1]]))</code></pre>
<pre><code>## [1] 2.353673e-14</code></pre>
<pre class="r"><code>max(abs(fit$beta[[2]][-1,] - fit.xtx$beta[[2]])) </code></pre>
<pre><code>## [1] 2.353673e-14</code></pre>
<pre class="r"><code>col.std &lt;- apply(xc, 2, sd)
fit.xtx.s &lt;- oem.xtx(xtx = xtx, xty = xty, 
                     scale.factor = col.std,
                     penalty = c(&quot;lasso&quot;, &quot;grp.lasso&quot;), 
                     groups = rep(1:20, each = 5))  </code></pre>
</div>
<div id="out-of-memory-computation" class="section level2">
<h2>Out-of-memory Computation</h2>
<p>The OEM package also provides functionality for on-disk computation with the <code>big.oem</code> function, allowing for fitting penalized regression models on datasets too large to fit in memory. The <code>big.oem</code> function uses the tools provided by the <code>bigmemory</code> package, so a big.matrix object must be used for the design matrix.</p>
<pre class="r"><code>set.seed(123)
nrows &lt;- 50000
ncols &lt;- 100
bkFile &lt;- &quot;bigmat.bk&quot;
descFile &lt;- &quot;bigmatk.desc&quot;
bigmat &lt;- filebacked.big.matrix(nrow=nrows, ncol=ncols, type=&quot;double&quot;,
                                backingfile=bkFile, backingpath=&quot;.&quot;,
                                descriptorfile=descFile,
                                dimnames=c(NULL,NULL))

# Each column value with be the column number multiplied by
# samples from a standard normal distribution.
set.seed(123)
for (i in 1:ncols) bigmat[,i] = rnorm(nrows)*i

yb &lt;- rnorm(nrows) + bigmat[,1] - bigmat[,2]

## out-of-memory computation
fit &lt;- big.oem(x = bigmat, y = yb, 
               penalty = c(&quot;lasso&quot;, &quot;grp.lasso&quot;), 
               groups = rep(1:20, each = 5))

## fitting with in-memory computation
fit2 &lt;- oem(x = bigmat[,], y = yb, 
            penalty = c(&quot;lasso&quot;, &quot;grp.lasso&quot;), 
            groups = rep(1:20, each = 5))   
           
max(abs(fit$beta[[1]] - fit2$beta[[1]]))            </code></pre>
<pre><code>## [1] 1.534783e-05</code></pre>
</div>
</div>
<div id="other-features" class="section level1">
<h1>Other Features</h1>
<div id="parallelization-via-openmp" class="section level2">
<h2>Parallelization via OpenMP</h2>
<p>Computational time can be reduced a little via OpenMP parallelization of the key computational steps of the OEM algorithm. Simply use the <code>ncores</code> argument to access parallelization. There is no need for the foreach package.</p>
<pre class="r"><code>nobsc  &lt;- 1e5
nvarsc &lt;- 500
xc &lt;- matrix(rnorm(nobsc * nvarsc), ncol = nvarsc)
yc &lt;- drop(xc %*% c(0.5, 0.5, -0.5, -0.5, 1, rep(0, nvarsc - 5))) + rnorm(nobsc, sd = 4)


system.time(fit &lt;- oem(x = xc, y = yc, 
                       penalty = c(&quot;lasso&quot;, &quot;grp.lasso&quot;), 
                       standardize = FALSE, intercept = FALSE,
                       groups = rep(1:20, each = 25)))</code></pre>
<pre><code>##    user  system elapsed 
##   2.167   0.137   2.305</code></pre>
<pre class="r"><code>system.time(fitp &lt;- oem(x = xc, y = yc, 
                        penalty = c(&quot;lasso&quot;, &quot;grp.lasso&quot;), 
                        standardize = FALSE, intercept = FALSE,
                        groups = rep(1:20, each = 25), ncores = 2))</code></pre>
<pre><code>##    user  system elapsed 
##   2.368   0.137   1.358</code></pre>
</div>
<div id="penalty-adjustment" class="section level2">
<h2>Penalty Adjustment</h2>
<p>If some variables should not be penalized, this can be specified through the use of the <code>penalty.factor</code> argument for all penalties other than the group lasso. For the group lasso, the group-specific weights can be modified by the <code>group.weights</code> argument. <code>penalty.factor</code> should be a vector of length equal to the number of columns in <code>x</code>. Each element in <code>penalty.factor</code> will be multiplied to the applied tuning parameter for each corresponding variable. For example, for a problem with 5 variables (<code>ncol(x) = 5</code>), setting <code>penalty.factor = c(1, 1, 1, 0, 0)</code> will effectively only allow penalization for the first three variables. The <code>group.weights</code> argument should be a vector with length equal to the number of groups. Similarly to <code>penalty.factor</code>, these weights will be multiplied to the penalty applied to each group. <code>penalty.factor</code> and <code>group.weights</code> can also be used to fit the adaptive lasso and adaptive group lasso, respectively.</p>
<p>The following example shows how to fit an adaptive lasso using <code>oem</code></p>
<pre class="r"><code>nobs  &lt;- 1e4
nvars &lt;- 102
x &lt;- matrix(rnorm(nobs * nvars), ncol = nvars)
y &lt;- drop(x %*% c(0.5, 0.5, -0.5, -0.5, 1, 0.5, rep(0, nvars - 6))) + rnorm(nobs, sd = 4)

lams &lt;- exp(seq(log(2.5), log(5e-5), length.out = 100L))

ols.estimates &lt;- coef(lm.fit(y = y, x = cbind(1, x)))[-1]

fit.adaptive &lt;- oem(x = x, y = y, penalty = c(&quot;lasso&quot;),
                    penalty.factor = 1 / abs(ols.estimates),
                    lambda = lams)

group.indicators &lt;- rep(1:34, each = 3)

## norms of OLS estimates for each group
group.norms      &lt;- sapply(1:34, function(idx) sqrt(sum((ols.estimates[group.indicators == idx]) ^ 2)))
fit.adaptive.grp &lt;- oem(x = x, y = y, penalty = c(&quot;grp.lasso&quot;),
                        group.weights = 1 / group.norms,
                        groups = group.indicators, 
                        lambda = lams)</code></pre>
<p><img src="../../../../post/2017-07-19-oem_cran_files/figure-html/unnamed-chunk-25-1.png" width="686.4" /></p>
</div>
</div>
<div id="more-information" class="section level1">
<h1>More Information</h1>
<p>For further information about <code>oem</code>, please visit:</p>
<ul>
<li>The oem site: <a href="http://jaredhuling.github.io/oem">casualinference.org/oem</a></li>
<li>The oem source code: <a href="https://github.com/jaredhuling/oem">github.com/jaredhuling/oem</a></li>
<li>The oem paper: <a href="http://www.tandfonline.com/doi/abs/10.1080/00401706.2015.1054436">oem paper</a></li>
</ul>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="../../../../feed.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="../../../../images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdn.bootcss.com/highlight.js/9.11.0/highlight.min.js"></script>



<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/r.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="../../../../js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-49091606-1', 'auto');
ga('send', 'pageview');
</script>

  </body>
</html>

